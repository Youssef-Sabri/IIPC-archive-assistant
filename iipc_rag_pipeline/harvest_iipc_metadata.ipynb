{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plfc2vRIfEM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215dab6e-5e8d-4753-dfa6-fd85eb8aff8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Harvesting metadata using: oai_dc\n",
            "‚úÖ Retrieved 587 metadata records.\n",
            "\n",
            "üîç Extracting full text from associated files...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üìÑ Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 587/587 [14:06<00:00,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Exported 587 records with extracted content to: iipcm_extracted_content.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import requests\n",
        "import csv\n",
        "import fitz  # PyMuPDF\n",
        "from io import BytesIO\n",
        "from pdf2image import convert_from_bytes\n",
        "import pytesseract\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Constants\n",
        "OAI_BASE = \"https://digital.library.unt.edu/oai/\"\n",
        "COLLECTION_SET = \"collection:IIPCM\"\n",
        "NAMESPACES = {\n",
        "    \"oai\": \"http://www.openarchives.org/OAI/2.0/\",\n",
        "    \"oai_dc\": \"http://www.openarchives.org/OAI/2.0/oai_dc/\",\n",
        "    \"dc\": \"http://purl.org/dc/elements/1.1/\",\n",
        "}\n",
        "OUTPUT_FILE = \"iipcm_extracted_content.csv\"\n",
        "\n",
        "def get_first(elements):\n",
        "    for el in elements:\n",
        "        if el.text:\n",
        "            return el.text.strip()\n",
        "    return \"\"\n",
        "\n",
        "def extract_pdf_text(pdf_url):\n",
        "    try:\n",
        "        response = requests.get(pdf_url, timeout=15)\n",
        "        if response.status_code != 200 or \"application/pdf\" not in response.headers.get(\"Content-Type\", \"\"):\n",
        "            return \"\"\n",
        "        pdf_bytes = response.content\n",
        "\n",
        "        # Try extracting using PyMuPDF\n",
        "        try:\n",
        "            doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
        "            text = \"\\n\".join([page.get_text() for page in doc])\n",
        "            if text.strip():\n",
        "                return text.strip()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Fallback to OCR\n",
        "        images = convert_from_bytes(pdf_bytes)\n",
        "        text = \"\"\n",
        "        for img in images:\n",
        "            text += pytesseract.image_to_string(img) + \"\\n\"\n",
        "        return text.strip()\n",
        "\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def extract_vtt_transcript(vtt_url):\n",
        "    try:\n",
        "        response = requests.get(vtt_url)\n",
        "        response.raise_for_status()\n",
        "        lines = response.text.splitlines()\n",
        "        transcript = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if (\n",
        "                line.startswith(\"WEBVTT\") or\n",
        "                line.startswith(\"NOTE\") or\n",
        "                \"-->\" in line or\n",
        "                line.isdigit() or\n",
        "                (\":\" in line and line.lower().startswith(\"vtt_\")) or\n",
        "                line == \"\"\n",
        "            ):\n",
        "                continue\n",
        "            transcript.append(line)\n",
        "\n",
        "        return \" \".join(transcript)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error parsing VTT from {vtt_url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def harvest_oai_records():\n",
        "    print(\"üîÅ Harvesting metadata using: oai_dc\")\n",
        "    records = []\n",
        "    token = None\n",
        "\n",
        "    while True:\n",
        "        params = {\n",
        "            \"verb\": \"ListRecords\",\n",
        "            \"metadataPrefix\": \"oai_dc\",\n",
        "            \"set\": COLLECTION_SET\n",
        "        } if not token else {\n",
        "            \"verb\": \"ListRecords\",\n",
        "            \"resumptionToken\": token\n",
        "        }\n",
        "\n",
        "        resp = requests.get(OAI_BASE, params=params)\n",
        "        root = ET.fromstring(resp.content)\n",
        "\n",
        "        for r in root.findall(\".//oai:record\", NAMESPACES):\n",
        "            header = r.find(\"oai:header\", NAMESPACES)\n",
        "            if header is None or header.attrib.get(\"status\") == \"deleted\":\n",
        "                continue\n",
        "\n",
        "            meta = r.find(\".//oai_dc:dc\", NAMESPACES)\n",
        "            if meta is None:\n",
        "                continue\n",
        "\n",
        "            identifiers = meta.findall(\"dc:identifier\", NAMESPACES)\n",
        "            ark_url = \"\"\n",
        "            for ident in identifiers:\n",
        "                if ident.text and \"ark:/67531/\" in ident.text:\n",
        "                    ark_url = ident.text.strip()\n",
        "                    break\n",
        "            if not ark_url:\n",
        "                continue\n",
        "\n",
        "            record = {\n",
        "                \"ark_url\": ark_url,\n",
        "                \"title\": get_first(meta.findall(\"dc:title\", NAMESPACES)),\n",
        "                \"date\": get_first(meta.findall(\"dc:date\", NAMESPACES)),\n",
        "                \"creator\": get_first(meta.findall(\"dc:creator\", NAMESPACES)),\n",
        "                \"subject\": \"; \".join([el.text.strip() for el in meta.findall(\"dc:subject\", NAMESPACES) if el.text]),\n",
        "                \"description\": get_first(meta.findall(\"dc:description\", NAMESPACES)),\n",
        "                \"item_type\": get_first(meta.findall(\"dc:type\", NAMESPACES)),\n",
        "            }\n",
        "\n",
        "            records.append(record)\n",
        "\n",
        "        token_el = root.find(\".//oai:resumptionToken\", NAMESPACES)\n",
        "        token = token_el.text.strip() if token_el is not None and token_el.text else None\n",
        "        if not token:\n",
        "            break\n",
        "\n",
        "    print(f\"‚úÖ Retrieved {len(records)} metadata records.\\n\")\n",
        "    return records\n",
        "\n",
        "def resolve_pdf_link(folder_url):\n",
        "    try:\n",
        "        res = requests.get(folder_url, timeout=15, allow_redirects=True)\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "        for link in soup.find_all(\"a\"):\n",
        "            href = link.get(\"href\", \"\")\n",
        "            if href.endswith(\".pdf\"):\n",
        "                return requests.compat.urljoin(folder_url, href)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error resolving PDF from folder: {folder_url} - {e}\")\n",
        "    return \"\"\n",
        "\n",
        "def process_record(record):\n",
        "    ark_url = record[\"ark_url\"]\n",
        "    item_type = record[\"item_type\"].lower()\n",
        "    text = \"\"\n",
        "    file_url = \"\"\n",
        "\n",
        "    try:\n",
        "        res = requests.get(ark_url, timeout=10)\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "        links = soup.find_all(\"a\")\n",
        "\n",
        "        for link in links:\n",
        "            href = link.get(\"href\", \"\")\n",
        "            full_url = requests.compat.urljoin(ark_url, href)\n",
        "\n",
        "            if item_type == \"video\" and href.endswith(\".vtt\"):\n",
        "                text = extract_vtt_transcript(full_url)\n",
        "                file_url = full_url\n",
        "                break\n",
        "\n",
        "            elif item_type != \"video\" and href.endswith(\".pdf\"):\n",
        "                # Redirect from folder? (e.g. ends with /high_res_d/)\n",
        "                if href.endswith(\"/\"):\n",
        "                    pdf_resolved = resolve_pdf_link(full_url)\n",
        "                    if pdf_resolved:\n",
        "                        text = extract_pdf_text(pdf_resolved)\n",
        "                        file_url = pdf_resolved\n",
        "                        break\n",
        "                else:\n",
        "                    text = extract_pdf_text(full_url)\n",
        "                    file_url = full_url\n",
        "                    break\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    record[\"source_url\"] = file_url\n",
        "    record[\"full_text\"] = text.replace(\"\\r\", \"\").replace(\"\\n\", \"\\\\n\")  # Clean for CSV\n",
        "    return record\n",
        "\n",
        "def main():\n",
        "    records = harvest_oai_records()\n",
        "    print(\"üîç Extracting full text from associated files...\\n\")\n",
        "    processed = []\n",
        "\n",
        "    for rec in tqdm(records, desc=\"üìÑ Processing\"):\n",
        "        processed.append(process_record(rec))\n",
        "\n",
        "    keys = [\"ark_url\", \"title\", \"date\", \"creator\", \"subject\", \"description\", \"item_type\", \"source_url\", \"full_text\"]\n",
        "    with open(OUTPUT_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(processed)\n",
        "\n",
        "    print(f\"\\n‚úÖ Exported {len(processed)} records with extracted content to: {OUTPUT_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}